{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from typing import Callable, Optional, Tuple\n",
    "from contextlib import nullcontext\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from adamp import AdamP\n",
    "from datetime import datetime \n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert'\n",
    "dataset_name = 'mbti'\n",
    "lr = 0.001 # default\n",
    "betas=(0.9, 0.999) \n",
    "weight_decay=1e-2\n",
    "n_epochs = 200\n",
    "early_stopping_window = 10\n",
    "\n",
    "path_data_dir = Path('..') / Path('data')\n",
    "path_dataset = path_data_dir / Path('split') / Path(f'{dataset_name}.csv')\n",
    "path_checkpoint_dir = Path('..') / Path('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>GROUP</th>\n",
       "      <th colspan=\"13\" halign=\"left\">CLS</th>\n",
       "      <th colspan=\"4\" halign=\"left\">STATS</th>\n",
       "      <th colspan=\"4\" halign=\"left\">TARGET</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FEATURE</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>...</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>NUM_CHARS</th>\n",
       "      <th>NUM_EMOJI</th>\n",
       "      <th>NUM_POSTS</th>\n",
       "      <th>NUM_UPPERCASED</th>\n",
       "      <th>mbtiEXT</th>\n",
       "      <th>mbtiJUD</th>\n",
       "      <th>mbtiSEN</th>\n",
       "      <th>mbtiTHI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUTHOR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-9221022384933360074</th>\n",
       "      <td>0.004908</td>\n",
       "      <td>-0.052056</td>\n",
       "      <td>-0.076416</td>\n",
       "      <td>0.136052</td>\n",
       "      <td>-0.131765</td>\n",
       "      <td>0.015147</td>\n",
       "      <td>-0.403912</td>\n",
       "      <td>-0.041273</td>\n",
       "      <td>-0.104842</td>\n",
       "      <td>-0.108374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210299</td>\n",
       "      <td>0.208951</td>\n",
       "      <td>82.842105</td>\n",
       "      <td>0.067669</td>\n",
       "      <td>133</td>\n",
       "      <td>7.924812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9220031623198266213</th>\n",
       "      <td>0.012287</td>\n",
       "      <td>-0.074256</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.190153</td>\n",
       "      <td>-0.158079</td>\n",
       "      <td>0.020834</td>\n",
       "      <td>-0.453263</td>\n",
       "      <td>0.046208</td>\n",
       "      <td>-0.154505</td>\n",
       "      <td>-0.102281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133708</td>\n",
       "      <td>0.280356</td>\n",
       "      <td>59.416667</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>120</td>\n",
       "      <td>2.958333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9219633155989415906</th>\n",
       "      <td>0.038200</td>\n",
       "      <td>-0.042412</td>\n",
       "      <td>0.025798</td>\n",
       "      <td>0.197942</td>\n",
       "      <td>-0.157285</td>\n",
       "      <td>0.081923</td>\n",
       "      <td>-0.349617</td>\n",
       "      <td>0.033938</td>\n",
       "      <td>-0.145553</td>\n",
       "      <td>-0.093792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216485</td>\n",
       "      <td>0.223513</td>\n",
       "      <td>178.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48</td>\n",
       "      <td>5.354167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9219237589017844173</th>\n",
       "      <td>0.075252</td>\n",
       "      <td>-0.001803</td>\n",
       "      <td>0.024392</td>\n",
       "      <td>0.180314</td>\n",
       "      <td>-0.205115</td>\n",
       "      <td>0.078434</td>\n",
       "      <td>-0.453093</td>\n",
       "      <td>0.042417</td>\n",
       "      <td>-0.187603</td>\n",
       "      <td>-0.015976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182507</td>\n",
       "      <td>0.178945</td>\n",
       "      <td>77.430380</td>\n",
       "      <td>0.221519</td>\n",
       "      <td>158</td>\n",
       "      <td>3.303797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9214568075844254832</th>\n",
       "      <td>0.057628</td>\n",
       "      <td>-0.087571</td>\n",
       "      <td>0.060698</td>\n",
       "      <td>0.196147</td>\n",
       "      <td>-0.166876</td>\n",
       "      <td>0.022728</td>\n",
       "      <td>-0.441546</td>\n",
       "      <td>0.062697</td>\n",
       "      <td>-0.161590</td>\n",
       "      <td>-0.092093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182287</td>\n",
       "      <td>0.315929</td>\n",
       "      <td>100.617284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81</td>\n",
       "      <td>6.716049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9220307502816513261</th>\n",
       "      <td>0.054631</td>\n",
       "      <td>-0.040838</td>\n",
       "      <td>0.048029</td>\n",
       "      <td>0.274093</td>\n",
       "      <td>-0.162984</td>\n",
       "      <td>0.100086</td>\n",
       "      <td>-0.424381</td>\n",
       "      <td>0.060659</td>\n",
       "      <td>-0.172998</td>\n",
       "      <td>-0.052123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206001</td>\n",
       "      <td>0.284869</td>\n",
       "      <td>125.840000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50</td>\n",
       "      <td>4.840000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9220556403022889385</th>\n",
       "      <td>-0.170737</td>\n",
       "      <td>-0.091014</td>\n",
       "      <td>-0.033050</td>\n",
       "      <td>0.133041</td>\n",
       "      <td>-0.146434</td>\n",
       "      <td>0.156010</td>\n",
       "      <td>-0.422089</td>\n",
       "      <td>-0.113840</td>\n",
       "      <td>-0.160334</td>\n",
       "      <td>-0.087500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261026</td>\n",
       "      <td>0.242038</td>\n",
       "      <td>134.159091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44</td>\n",
       "      <td>6.681818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9221651641191792423</th>\n",
       "      <td>0.035870</td>\n",
       "      <td>-0.075790</td>\n",
       "      <td>0.032501</td>\n",
       "      <td>0.209679</td>\n",
       "      <td>-0.196971</td>\n",
       "      <td>0.039044</td>\n",
       "      <td>-0.391962</td>\n",
       "      <td>0.067448</td>\n",
       "      <td>-0.152983</td>\n",
       "      <td>-0.029710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217194</td>\n",
       "      <td>0.289530</td>\n",
       "      <td>173.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>4.976190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9222607780732095571</th>\n",
       "      <td>-0.007086</td>\n",
       "      <td>-0.089152</td>\n",
       "      <td>0.079623</td>\n",
       "      <td>0.223943</td>\n",
       "      <td>-0.160965</td>\n",
       "      <td>0.143289</td>\n",
       "      <td>-0.386648</td>\n",
       "      <td>0.021990</td>\n",
       "      <td>-0.167608</td>\n",
       "      <td>0.045628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167094</td>\n",
       "      <td>0.210510</td>\n",
       "      <td>186.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50</td>\n",
       "      <td>5.980000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9222919918896192253</th>\n",
       "      <td>0.024127</td>\n",
       "      <td>-0.043071</td>\n",
       "      <td>0.058259</td>\n",
       "      <td>0.218798</td>\n",
       "      <td>-0.152382</td>\n",
       "      <td>0.041907</td>\n",
       "      <td>-0.343285</td>\n",
       "      <td>0.057455</td>\n",
       "      <td>-0.134904</td>\n",
       "      <td>-0.057263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223239</td>\n",
       "      <td>0.283026</td>\n",
       "      <td>86.656250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17741 rows × 776 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "GROUP                      CLS                                          \\\n",
       "FEATURE                      0         1        10       100       101   \n",
       "AUTHOR                                                                   \n",
       "-9221022384933360074  0.004908 -0.052056 -0.076416  0.136052 -0.131765   \n",
       "-9220031623198266213  0.012287 -0.074256  0.020761  0.190153 -0.158079   \n",
       "-9219633155989415906  0.038200 -0.042412  0.025798  0.197942 -0.157285   \n",
       "-9219237589017844173  0.075252 -0.001803  0.024392  0.180314 -0.205115   \n",
       "-9214568075844254832  0.057628 -0.087571  0.060698  0.196147 -0.166876   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       " 9220307502816513261  0.054631 -0.040838  0.048029  0.274093 -0.162984   \n",
       " 9220556403022889385 -0.170737 -0.091014 -0.033050  0.133041 -0.146434   \n",
       " 9221651641191792423  0.035870 -0.075790  0.032501  0.209679 -0.196971   \n",
       " 9222607780732095571 -0.007086 -0.089152  0.079623  0.223943 -0.160965   \n",
       " 9222919918896192253  0.024127 -0.043071  0.058259  0.218798 -0.152382   \n",
       "\n",
       "GROUP                                                                   ...  \\\n",
       "FEATURE                    102       103       104       105       106  ...   \n",
       "AUTHOR                                                                  ...   \n",
       "-9221022384933360074  0.015147 -0.403912 -0.041273 -0.104842 -0.108374  ...   \n",
       "-9220031623198266213  0.020834 -0.453263  0.046208 -0.154505 -0.102281  ...   \n",
       "-9219633155989415906  0.081923 -0.349617  0.033938 -0.145553 -0.093792  ...   \n",
       "-9219237589017844173  0.078434 -0.453093  0.042417 -0.187603 -0.015976  ...   \n",
       "-9214568075844254832  0.022728 -0.441546  0.062697 -0.161590 -0.092093  ...   \n",
       "...                        ...       ...       ...       ...       ...  ...   \n",
       " 9220307502816513261  0.100086 -0.424381  0.060659 -0.172998 -0.052123  ...   \n",
       " 9220556403022889385  0.156010 -0.422089 -0.113840 -0.160334 -0.087500  ...   \n",
       " 9221651641191792423  0.039044 -0.391962  0.067448 -0.152983 -0.029710  ...   \n",
       " 9222607780732095571  0.143289 -0.386648  0.021990 -0.167608  0.045628  ...   \n",
       " 9222919918896192253  0.041907 -0.343285  0.057455 -0.134904 -0.057263  ...   \n",
       "\n",
       "GROUP                                          STATS                      \\\n",
       "FEATURE                     98        99   NUM_CHARS NUM_EMOJI NUM_POSTS   \n",
       "AUTHOR                                                                     \n",
       "-9221022384933360074  0.210299  0.208951   82.842105  0.067669       133   \n",
       "-9220031623198266213  0.133708  0.280356   59.416667  0.341667       120   \n",
       "-9219633155989415906  0.216485  0.223513  178.041667  0.000000        48   \n",
       "-9219237589017844173  0.182507  0.178945   77.430380  0.221519       158   \n",
       "-9214568075844254832  0.182287  0.315929  100.617284  0.000000        81   \n",
       "...                        ...       ...         ...       ...       ...   \n",
       " 9220307502816513261  0.206001  0.284869  125.840000  0.000000        50   \n",
       " 9220556403022889385  0.261026  0.242038  134.159091  0.000000        44   \n",
       " 9221651641191792423  0.217194  0.289530  173.071429  0.000000        42   \n",
       " 9222607780732095571  0.167094  0.210510  186.740000  0.000000        50   \n",
       " 9222919918896192253  0.223239  0.283026   86.656250  0.000000        32   \n",
       "\n",
       "GROUP                                TARGET                          \n",
       "FEATURE              NUM_UPPERCASED mbtiEXT mbtiJUD mbtiSEN mbtiTHI  \n",
       "AUTHOR                                                               \n",
       "-9221022384933360074       7.924812     0.0     0.0     0.0     1.0  \n",
       "-9220031623198266213       2.958333     0.0     1.0     1.0     1.0  \n",
       "-9219633155989415906       5.354167     0.0     0.0     0.0     1.0  \n",
       "-9219237589017844173       3.303797     0.0     0.0     0.0     0.0  \n",
       "-9214568075844254832       6.716049     0.0     0.0     0.0     1.0  \n",
       "...                             ...     ...     ...     ...     ...  \n",
       " 9220307502816513261       4.840000     1.0     0.0     0.0     0.0  \n",
       " 9220556403022889385       6.681818     0.0     1.0     0.0     1.0  \n",
       " 9221651641191792423       4.976190     0.0     0.0     0.0     0.0  \n",
       " 9222607780732095571       5.980000     0.0     1.0     0.0     0.0  \n",
       " 9222919918896192253       5.750000     0.0     0.0     0.0     1.0  \n",
       "\n",
       "[17741 rows x 776 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path_dataset, header=[0, 1], index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPDataset(Dataset):\n",
    "  def __init__(self, data:pd.DataFrame):\n",
    "    self.data_x = torch.Tensor(data.drop([\"TARGET\"], axis=1).values)\n",
    "    self.data_y = torch.Tensor(data[\"TARGET\"].values)\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    return self.data_y.shape[0]\n",
    "  \n",
    "  def __getitem__(self, index:int) -> torch.Tensor:\n",
    "    return self.data_x[index], self.data_y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PPDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset: Dataset, train_size: float, test_size: float):\n",
    "  total_length = len(dataset)\n",
    "  train_length = int(train_size * total_length)\n",
    "  test_length = int(test_size * total_length)\n",
    "  val_length = total_length - (train_length + test_length)\n",
    "  return random_split(dataset, [train_length, test_length, val_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds, val_ds = split_dataset(dataset, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = {\n",
    "  'batch_size': 1024,\n",
    "  'num_workers': 1,\n",
    "  'shuffle': True,\n",
    "  'pin_memory': False,\n",
    "  'drop_last': True\n",
    "}\n",
    "\n",
    "test_args = {\n",
    "  'batch_size': 1024,\n",
    "  'num_workers': 1,\n",
    "  'shuffle': False,\n",
    "  'pin_memory': False,\n",
    "  'drop_last': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, **train_args)\n",
    "test_dl = DataLoader(test_ds, **test_args)\n",
    "val_dl = DataLoader(val_ds, **test_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, hiddens:list[int], dropout_percent=0.5, final='Sigmoid'):\n",
    "    super(Decoder, self).__init__()\n",
    "    layers = []\n",
    "    for i in range(len(hiddens) - 1):\n",
    "        layers.append(nn.Linear(hiddens[i], hiddens[i + 1]))\n",
    "        if i < len(hiddens) - 2:\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_percent))\n",
    "    if final.lower() == 'sigmoid': layers.append(nn.Sigmoid())\n",
    "    elif final.lower() == 'relu': layers.append(nn.ReLU())\n",
    "    self.model = nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "    return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_epoch(epoch_type:str, dl: DataLoader, model: nn.Module, loss_fn:Callable, optimizer:Optimizer=None) -> Tuple[Optional[float], float]:\n",
    "    valid_epoch_types = ('train', 'test', 'val', 'validation')\n",
    "    if epoch_type.lower() not in valid_epoch_types: raise TypeError(f'Argument \"epoch_type\" must be one of {valid_epoch_types}')\n",
    "    model.train() if epoch_type == 'train' else model.eval()\n",
    "    context_manager = torch.no_grad() if epoch_type != 'train' else nullcontext()\n",
    "\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with context_manager:\n",
    "      for data_x, data_y in dl:\n",
    "          data_x = data_x.to(device)\n",
    "          data_y = data_y.to(device)\n",
    "\n",
    "          if epoch_type == 'train': optimizer.zero_grad()\n",
    "\n",
    "          predictions = model(data_x)\n",
    "\n",
    "          loss = loss_fn(predictions, data_y) if epoch_type != 'test' else None\n",
    "\n",
    "          if epoch_type == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "          preds = (predictions > 0.5).float()\n",
    "          running_corrects += torch.sum(preds == data_y).item()\n",
    "          if epoch_type != 'test': running_loss += loss.item() * data_x.size(0)\n",
    "          total_samples += data_y.numel()\n",
    "\n",
    "    avg_loss = running_loss / total_samples if epoch_type != 'test' else None\n",
    "    avg_acc = running_corrects / total_samples\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:nn.Module, train:DataLoader, val:DataLoader, test:DataLoader, optimizer:Optimizer, loss_fn:Callable, n_epochs:int, checkpoint_name:str='default', early_stopping_window=5):\n",
    "  start = datetime.now()\n",
    "  model_path = path_checkpoint_dir / Path(f'{checkpoint_name}.pth')\n",
    "\n",
    "  best_epoch = -1\n",
    "  best_vacc = float('-inf')\n",
    "\n",
    "  train_loop = trange(n_epochs, desc='Training', leave=True)\n",
    "  for epoch in train_loop:\n",
    "\n",
    "    # train\n",
    "    avg_loss, avg_acc = handle_epoch('train', train, model, loss_fn=loss_fn, optimizer=optimizer) \n",
    "\n",
    "    # val\n",
    "    avg_vloss, avg_vacc = handle_epoch('val', val, model, loss_fn=loss_fn)\n",
    "    \n",
    "    train_loop.set_description(f'EPOCH {epoch}: Train loss: {avg_loss:.3f}, Val loss: {avg_vloss:.3f} \\t Train acc: {avg_acc:.3f}, Val acc: {avg_vacc:.3f}')\n",
    "\n",
    "    if avg_vacc > best_vacc:\n",
    "      best_vacc = avg_vacc\n",
    "      best_epoch = epoch\n",
    "      torch.save(model.state_dict(), model_path)\n",
    "    elif epoch - best_epoch > early_stopping_window:\n",
    "      tqdm.write(f'Early stopping with best validation accuracy {best_vacc*100:.3f}%')\n",
    "      break\n",
    "  \n",
    "  _, avg_tacc = handle_epoch('test', test, model, loss_fn=loss_fn) \n",
    "\n",
    "  end = datetime.now()\n",
    "  total_time = end - start\n",
    "\n",
    "  tqdm.write(f'Training finished after {total_time}. Test accuracy {avg_tacc*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=772, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): Linear(in_features=512, out_features=4, bias=True)\n",
       "    (10): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = 4\n",
    "model = Decoder([data.shape[1]-classes, 2048, 2048, 512, classes])\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = AdamP(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, train_dl, val_dl, test_dl, optimizer, loss_fn, n_epochs, checkpoint_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmbti-test\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "train(model, train_dl, val_dl, test_dl, optimizer, loss_fn, n_epochs, checkpoint_name='mbti-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1: Train loss: 0.1488, Val loss: 0.1498. Low dropout\n",
    "# 2: High dropout: Early stopped training at 20 with best accuracy 66.1376953125%\n",
    "# 3: 2048: Early stopped training at 15 with best accuracy 66.0889%\n",
    "# Decoder([data.shape[1]-classes, 4096, 4096, 1024, 256, classes]): EPOCH 15 \t Train loss: 0.152, Val loss: 0.151 \t Train acc: 0.672, Val acc: 0.667\n",
    "# Decoder([data.shape[1]-classes, 2048, 2048, 1024, 256, classes]) EPOCH 11 \t Train loss: 0.151, Val loss: 0.152 \t Train acc: 0.673, Val acc: 0.665 [01:30<25:56,  8.24s/it]\n",
    "# EPOCH 13: Train loss: 0.151, Val loss: 0.151 \t Train acc: 0.672, Val acc: 0.669:   6%|▋         | 13/200 [01:30<21:48,  7.00s/it]\n",
    "# Decoder([data.shape[1]-classes, 2048, 2048, 512, classes]): Training finished after 0:01:31.861364. Test accuracy 66.919%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
