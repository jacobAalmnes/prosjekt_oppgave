{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobalmnes/Documents/Personality-Prediction-master/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizerFast, DistilBertConfig, Trainer, TrainingArguments, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "    super(ClassificationHead, self).__init__()\n",
    "    self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.relu(self.linear1(x))\n",
    "    x = self.sigmoid(self.linear2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutliClassDistilBert(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MutliClassDistilBert, self).__init__()\n",
    "\n",
    "    self.base = DistilBertModel.from_pretrained('distilbert-base-cased', output_hidden_states=True)\n",
    "\n",
    "    self.head_I_E = ClassificationHead(768, 320, 1)\n",
    "    self.head_N_S = ClassificationHead(768, 320, 1)\n",
    "    self.head_T_F = ClassificationHead(768, 320, 1)\n",
    "    self.head_J_P = ClassificationHead(768, 320, 1)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    inputs = outputs.last_hidden_state\n",
    "    I_E = self.head_I_E(inputs)\n",
    "    N_S = self.head_N_S(inputs)\n",
    "    T_F = self.head_T_F(inputs)\n",
    "    J_P = self.head_J_P(inputs)\n",
    "\n",
    "    return I_E, N_S, T_F, J_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MutliClassDistilBert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv(\"../data/pandora/pandora_profiles/author_profiles.csv\")\n",
    "comments = pd.read_csv(\"../data/pandora/pandora_comments/all_comments_since_2015.csv\", nrows=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors['I/E'] = authors['mbti'].str[0].apply(lambda x: 1 if x == 'e' else 0)\n",
    "authors['N/S'] = authors['mbti'].str[1].apply(lambda x: 1 if x == 's' else 0)\n",
    "authors['T/F'] = authors['mbti'].str[2].apply(lambda x: 1 if x == 'f' else 0)\n",
    "authors['J/P'] = authors['mbti'].str[3].apply(lambda x: 1 if x == 'p' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = authors[['author', 'introverted', 'intuitive', 'thinking', 'perceiving']]\n",
    "comments = comments[['author', 'body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandora = pd.merge(authors, comments, on='author')\n",
    "pandora.drop('author', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandora = Dataset.from_pandas(pandora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['introverted', 'intuitive', 'thinking', 'perceiving', 'body'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['introverted', 'intuitive', 'thinking', 'perceiving', 'body'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['introverted', 'intuitive', 'thinking', 'perceiving', 'body'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandora.shuffle(seed=42)\n",
    "train_test_val = pandora.train_test_split(test_size=0.2)\n",
    "test_val = train_test_val['test'].train_test_split(test_size=0.5)\n",
    "pandora = DatasetDict({\n",
    "  'train': train_test_val['train'],\n",
    "  'val': test_val['train'],\n",
    "  'test': test_val['test']\n",
    "})\n",
    "pandora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16000/16000 [00:00<00:00, 31729.28 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 28933.92 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 29192.69 examples/s]\n",
      "Map: 100%|██████████| 16000/16000 [00:01<00:00, 8309.83 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 6881.91 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 8887.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def combine_labels(example):\n",
    "  example['labels'] = [example['introverted'], example['intuitive'], example['thinking'], example['perceiving']]\n",
    "\n",
    "pandora = pandora.map(combine_labels)\n",
    "\n",
    "pandora = pandora.map(\n",
    "  lambda example: tokenizer(example['body'], padding=True, truncation=True),\n",
    "  batched=True,\n",
    "  batch_size=16\n",
    ")\n",
    "\n",
    "columns_to_remove = ['introverted', 'intuitive', 'thinking', 'perceiving', 'body']\n",
    "pandora = pandora.remove_columns(columns_to_remove)\n",
    "pandora.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(pandora['train'], batch_size=16)\n",
    "val_dataloader = DataLoader(pandora['val'], batch_size=16)\n",
    "test_dataloader = DataLoader(pandora['test'], batch_size=16)\n",
    "pandora = 0\n",
    "\n",
    "eval_dataloader = val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobalmnes/Documents/Personality-Prediction-master/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jacobalmnes/Documents/Personality-Prediction-master/tutorials/transformers-start.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobalmnes/Documents/Personality-Prediction-master/tutorials/transformers-start.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m   \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobalmnes/Documents/Personality-Prediction-master/tutorials/transformers-start.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jacobalmnes/Documents/Personality-Prediction-master/tutorials/transformers-start.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   val_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m output\u001b[39m.\u001b[39;49mloss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobalmnes/Documents/Personality-Prediction-master/tutorials/transformers-start.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m avg_val_loss \u001b[39m=\u001b[39m val_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(eval_dataloader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobalmnes/Documents/Personality-Prediction-master/tutorials/transformers-start.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAverage Validation Loss: \u001b[39m\u001b[39m{\u001b[39;00mavg_val_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "num_training_steps = 3 * len(train_dataloader)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "loss_func = CrossEntropyLoss()\n",
    "lr_scheduler= get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  for batch in train_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**batch)\n",
    "    lr_scheduler.step()\n",
    "    optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  val_loss = 0\n",
    "  for batch_i, batch in enumerate(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "      output = model(**batch)\n",
    "    val_loss += output.loss\n",
    "\n",
    "  avg_val_loss = val_loss / len(eval_dataloader)\n",
    "  print(f'Average Validation Loss: {avg_val_loss}')\n",
    "  if avg_val_loss < best_val_loss:\n",
    "    print(f\"Saving Checkpoint..\")\n",
    "    best_val_loss = avg_val_loss\n",
    "    torch.save({\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': model.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'val_loss': best_val_loss\n",
    "      },\n",
    "      f'checkpoints/epoch_{epoch}.pt  '\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
