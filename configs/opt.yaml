seed: 42

study:
  load_if_exists: False

split:
  train: 0.8
  test: 0.1

dataloaders:
  train:
    num_workers: 2
    pin_memory: True
    batch_size: 1024
    shuffle: True
    drop_last: True
  test:
    num_workers: 2
    pin_memory: True
    batch_size: 1024
    shuffle: False
    drop_last: False
  val:
    num_workers: 2
    pin_memory: True
    batch_size: 1024
    shuffle: False
    drop_last: False

metrics:
  class: 'Acc' 
  class-args: 
    criteria: 'hamming'
    threshold: 0.50
  reg: 'MAE'

training:
  checkpoint_name: 'unified'
  epochs: 150
  patience: 10

optim_param:
  optim: 'adamp'
  weight_decay: 0.01
  betas: [0.99, 0.999]

scheduler_param:
  scheduler: 'step'

experiment:
  
  architecture:
    HPS: {} # Hard parameter sharing
    MMoE: 
      img_size: [780, 1] # TODO: UPDATE
      num_experts: [4, 5, 5] # Docs incorrect..
    # CGC: #Doesn't work
    #   img_size: [780, 1] # TODO: UPDATE
    #   num_experts: [4, 5, 5]
    DSelect_k:
      img_size: [780, 1] # TODO: UPDATE
      num_experts: [4, 5, 5]
      num_nonzeros: 3
      kgamma: 1.0
  weighting: 
    EW: {}
    GradNorm:
      alpha: 1.5
    MGDA:
      mgda_gn: "none"
    UW: {}
    DWA:
      T: 2.0
    GLS: {}
    GradDrop:
      leak: 0.0
        # Does not work with optuna    IMTL: {}
    RLW: {}
    Aligned_MTL: {}
    #DB_MTL:  Not supported with rep grad
      #DB_beta: 0
      #DB_beta_sigma: 1
  combine-emb: ['xlnet', 'mdeberta', 'tw-roberta']
